{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Lab 4 Keras Tuner \n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CN31THTMs8T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using Keras Tuner and Tensorflow"
      ],
      "metadata": {
        "id": "cKKgLKCCqkkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "KerasTuner is a general-purpose hyperparameter tuning library. It has strong integration with Keras workflows, but it isn't limited to them: you could use it to tune scikit-learn models, or anything else. In this lab, you will see how to tune model architecture, training process, and data preprocessing steps with KerasTuner."
      ],
      "metadata": {
        "id": "CFBbo4fFteFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some advanced hyperparameter tuning algorithms, including Random serach tuner, Bayesian hyperparameter optimization, Hyperband, Sklearn tuner. All of these are implemented inside the [keras tuner package](https://keras.io/keras_tuner/)."
      ],
      "metadata": {
        "id": "dttZqgByqsSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Keras Tuner\n",
        "\n",
        "\n",
        "1.   Ease of use\n",
        "\n",
        "2. Integrates into your existing deep learning training pipeline with minimal code changes\n",
        "3. Implements novel hyperparameter tuning algorithms\n",
        "4. Can boost accuracy with minimal effort on your part\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9M8paWJjr7cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner -q"
      ],
      "metadata": {
        "id": "DZAx3sUcux2P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa3be41-8ed3-4fb8-e04e-44de4e0298f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 18.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refresher\n",
        "\n",
        "## Tune the model architecture\n",
        "The first thing we need to do is writing a function, which returns a compiled Keras model. It takes an argument hp for defining the hyperparameters while building the model.\n",
        "\n",
        "## Define the search space\n",
        "In the following code example, we define a Keras model with two Dense layers.\n",
        "\n",
        "**Task**: #number of units in the first Dense layer.(Tuning parameter)\n",
        "\n",
        "**Resolve**: Define an integer hyperparameter with hp.Int('units', min_value=32, max_value=512, step=32), whose range is from 32 to 512 inclusive. When sampling from it, the minimum step for walking through the interval is 32.\n",
        "\n",
        "Hint ⚓ [Link](https://keras.io/api/keras_tuner/) "
      ],
      "metadata": {
        "id": "4lMYqLbetrq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            # Define the hyperparameter.\n",
        "            units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "wkfpGgvbu1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#quickly test if the model builds successfully.\n",
        "import keras_tuner\n",
        "\n",
        "build_model(keras_tuner.HyperParameters())"
      ],
      "metadata": {
        "id": "I1LNUxbmu5xn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ee6bac-730f-4e3b-d707-7a4c05d67d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7fcaaef46150>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=3,  #The total number of trials to run during the search.\n",
        "    executions_per_trial=2, #The number of models that should be built and fit for each trial.\n",
        "    overwrite=True, #Control whether to overwrite the previous results, overwrite=True to start a new search and ignore any previous results.\n",
        "    directory=\"my_dir\",#A path to a directory for storing the search results.\n",
        "    project_name=\"helloworld\", #The name of the sub-directory in the directory.\n",
        ")"
      ],
      "metadata": {
        "id": "i-T3Ext7vUFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search_space_summary() #print a summary of the search space"
      ],
      "metadata": {
        "id": "MZUAVNgrvW6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691202fc-1259-4496-aee4-300d56b02f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 1\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Before starting the search, let's prepare the MNIST dataset.\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x[:-10000]\n",
        "x_val = x[-10000:]\n",
        "y_train = y[:-10000]\n",
        "y_val = y[-10000:]\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "t15dCsT_vasG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ca18049-8e8e-4735-dd94-2d5e1fffdfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, start the search for the best hyperparameter configuration. All the arguments passed to search is passed to model.fit() in each execution. Remember to pass validation_data to evaluate the model."
      ],
      "metadata": {
        "id": "CopCCHNrvvkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "JFLTleCRvdFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f215b2b-1831-45d1-c501-6944ee38722e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3 Complete [00h 00m 42s]\n",
            "val_accuracy: 0.9739999771118164\n",
            "\n",
            "Best val_accuracy So Far: 0.9739999771118164\n",
            "Total elapsed time: 00h 01m 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary() # print a summary of the search results."
      ],
      "metadata": {
        "id": "dcGH7sh_w2jG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6774a489-4e4a-4367-a8a1-19ad827efcd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in my_dir/helloworld\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fca35e7c0d0>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "Score: 0.9739999771118164\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "Score: 0.9736000001430511\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "Score: 0.971750020980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query the results\n",
        "When search is over, you can retrieve the best model(s). The model is saved at its best performing epoch evaluated on the validation_data."
      ],
      "metadata": {
        "id": "2-fMWMgfty4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "# Build the model.\n",
        "# Needed for `Sequential` without specified `input_shape`.\n",
        "best_model.build(input_shape=(None, 28, 28))\n",
        "best_model.summary()"
      ],
      "metadata": {
        "id": "47745G3cwqe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab48c7e-9428-4267-d5ae-8f6476517a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 480)               376800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                4810      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 381,610\n",
            "Trainable params: 381,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will find detailed logs, checkpoints, etc, in the folder my_dir/helloworld, i.e. directory/project_name.\n",
        "\n",
        "You can also visualize the tuning results using TensorBoard and HParams plugin. For more information, please following [this link](https://keras.io/guides/keras_tuner/visualize_tuning/)."
      ],
      "metadata": {
        "id": "pcS2Mnniw-H1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain the model\n",
        "If you want to train the model with the entire dataset, you may retrieve the best hyperparameters and retrain the model by yourself."
      ],
      "metadata": {
        "id": "gsMrIynNxGc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 2 hyperparameters.\n",
        "best_hps = tuner.get_best_hyperparameters(5)\n",
        "# Build the model with the best hp.\n",
        "model = build_model(best_hps[0])\n",
        "# Fit with the entire dataset.\n",
        "x_all = np.concatenate((x_train, x_val))\n",
        "y_all = np.concatenate((y_train, y_val))\n",
        "model.fit(x=x_all, y=y_all, epochs=1)"
      ],
      "metadata": {
        "id": "zWoUIAKsxIQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b526a261-8a0b-4aec-81ce-734281f8234e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2031 - accuracy: 0.9406\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fca35ef8a90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Assisgnment\n",
        "\n",
        "\n",
        "\n",
        "1.   Add more parameters for hyperparameter tuning\n",
        "\n",
        "\n",
        "> **Task:1** Activation function[\"relu\", \"tanh\"], learning_rate(1e-4, 1e-2), Dropout(0.15), #number of layers(2, 10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ngq1zq7bxpQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <put your resolve here >\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    layer_num = hp.Int(\"layer\", min_value=1, max_value=9, step=1)\n",
        "    for _ in range(layer_num):\n",
        "      model.add(\n",
        "          layers.Dense(\n",
        "              units=hp.Int(\"units\", min_value=32, max_value=512, step=32),\n",
        "              activation=hp.Choice(\"activation\", values = [\"relu\", \"tanh\"]),\n",
        "          )\n",
        "      )\n",
        "\n",
        "    model.add(keras.layers.Dropout(0.15))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "6vK3hjlazLvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> **Task:1.2** Use Hyperband tuner instead of randomsearch\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_w8lCE3zRp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = keras_tuner.Hyperband(build_model,objective='val_accuracy',max_epochs=5,factor=3,directory=\"/content/keras_tuner_test\",project_name= \"kt_hyperband\")"
      ],
      "metadata": {
        "id": "qLpGyob13VaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Task:1.3** Tune data preprocessing step, by normalizing the data before training the model, do data shuffling in each epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "b2tVulW4x0SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <put your resolve here >\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train, X_rem, y_train, y_rem = train_test_split(x,y, train_size=0.8)\n",
        "\n",
        "# Now since we want the valid and test size to be equal (10% each of overall data). \n",
        "# we have to define valid_size=0.5 (that is 50% of remaining data)\n",
        "test_size = 0.5\n",
        "x_val, x_test, y_val, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\n",
        "\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
        "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "C_koULhw4eBG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> **Task :1.4** Compare results for each task, and find the maximum accuracy among all the tasks performed on mnist dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "zIyzrV_64hQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <put your resolve here >\n",
        "tuner.search(x_train, y_train, epochs=2, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "MG_yIr8B43QV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cc2746-8ed7-478c-9891-4b3a80fb212c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 42s]\n",
            "val_accuracy: 0.6635000109672546\n",
            "\n",
            "Best val_accuracy So Far: 0.8425999879837036\n",
            "Total elapsed time: 00h 03m 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poioQuyi17C8",
        "outputId": "f768c563-57a5-424a-ce9d-35aa229773f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in /content/keras_tuner_test/kt_hyperband\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fca3425f0d0>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 6\n",
            "activation: tanh\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0002\n",
            "Score: 0.8425999879837036\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 6\n",
            "activation: tanh\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.8022000193595886\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 8\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.7430999875068665\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 5\n",
            "activation: relu\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.6976000070571899\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 8\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0000\n",
            "Score: 0.6639000177383423\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 4\n",
            "activation: relu\n",
            "learning_rate: 0.001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.6635000109672546\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 2\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.5558000206947327\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 4\n",
            "activation: tanh\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.5220999717712402\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 2\n",
            "activation: tanh\n",
            "learning_rate: 0.0001\n",
            "tuner/epochs: 5\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 0.3046000003814697\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 3\n",
            "activation: relu\n",
            "learning_rate: 0.01\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Score: 0.10899999737739563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top 2 models.\n",
        "models = tuner.get_best_models(num_models=2)\n",
        "best_model = models[0]\n",
        "# Build the model.\n",
        "# Needed for `Sequential` without specified `input_shape`.\n",
        "best_model.build(input_shape=(None, 28, 28))\n",
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9U85Ksl17zl",
        "outputId": "d222a99b-7049-415d-c8dd-755c059945e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 4710      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 42        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.kernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dense_4 (Dense)             (None, 6)                 42        \n",
            "                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-0.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dense_5 (Dense)             (None, 6)                 42        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.kernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer_with_weights-1.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dropout (Dropout)           (None, 6)                 0         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dense_6 (Dense)             (None, 10)                70        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 4,990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 4,990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-trainable params: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.kernel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Task 2:** Compare keras tuner with PSO based Algorithm in terms of number of fitness function evaluation, exploration time and quality of solution"
      ],
      "metadata": {
        "id": "-byUnz8erc17"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YeGAeAjIrbxY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}